<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc-markdown-css-theme" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>IADB: MNIST tutorial</title>
  <link rel="stylesheet" href="css/theme.css" />
  <link rel="stylesheet" href="css/skylighting-paper-theme.css" />
</head>
<body>

<header>
<h1 class="title">IADB: MNIST tutorial</h1>
<blockquote class="metadata">
</blockquote>
</header>


<main> 
<p>We provide a simple tutorial for <a
href="https://ggx-research.github.io/publication/2023/05/10/publication-iadb.html">Iterative
ùõº-(de)Blending</a> applied to MNIST.</p>
<p>We provide a <a
href="https://github.com/tchambon/posts/blob/main/iadb-MNIST/IADB_MNIST.py">Python
code</a> and explain how it works below.</p>
<h2 id="data-loading">Data loading</h2>
<p>The objective is to create a mapping between Gaussian noise (the
<span class="math inline"><em>x</em><sub>0</sub></span>) and MNIST
images (the <span class="math inline"><em>x</em><sub>1</sub></span>). We
start by loading MNIST images as a torchvision dataset:</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># dataset    </span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>batchsize <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> torchvision.datasets.MNIST(<span class="st">&#39;/files/&#39;</span>, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>                            transform<span class="op">=</span>torchvision.transforms.Compose([</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>                            torchvision.transforms.ToTensor(),</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>                            torchvision.transforms.Normalize(</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>                                (<span class="fl">0.1307</span>,), (<span class="fl">0.3081</span>,))</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>                            ]))</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>dataloader <span class="op">=</span> DataLoader(dataset, batch_size<span class="op">=</span>batchsize, num_workers<span class="op">=</span><span class="dv">4</span>, drop_last<span class="op">=</span><span class="va">True</span>, shuffle<span class="op">=</span><span class="va">True</span>)   </span></code></pre></div>
<h2 id="neural-network">Neural network</h2>
<p>We train a neural network to learn the differential term (the
tangent) of the mapping between the samples <span
class="math inline"><em>x</em><sub>0</sub></span> and <span
class="math inline"><em>x</em><sub>1</sub></span>. We use a simple Unet
with 3 down/up-scaling layers and skip connections.</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># simple Unet architecture</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Unet(torch.nn.Module):</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Unet, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># block down 1</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block1_conv1 <span class="op">=</span> torch.nn.Conv2d( <span class="dv">2</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>,<span class="dv">3</span>), padding<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">1</span>), padding_mode<span class="op">=</span><span class="st">&#39;zeros&#39;</span>, stride<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block1_conv2 <span class="op">=</span> torch.nn.Conv2d(<span class="dv">64</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>,<span class="dv">3</span>), padding<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">1</span>), padding_mode<span class="op">=</span><span class="st">&#39;zeros&#39;</span>, stride<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># block down 2</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block2_conv1 <span class="op">=</span> torch.nn.Conv2d(<span class="dv">64</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>,<span class="dv">3</span>), padding<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">1</span>), padding_mode<span class="op">=</span><span class="st">&#39;zeros&#39;</span>, stride<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block2_conv2 <span class="op">=</span> torch.nn.Conv2d(<span class="dv">64</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>,<span class="dv">3</span>), padding<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">1</span>), padding_mode<span class="op">=</span><span class="st">&#39;zeros&#39;</span>, stride<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># block down 3</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block3_conv1 <span class="op">=</span> torch.nn.Conv2d(<span class="dv">64</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>,<span class="dv">3</span>), padding<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">1</span>), padding_mode<span class="op">=</span><span class="st">&#39;zeros&#39;</span>, stride<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block3_conv2 <span class="op">=</span> torch.nn.Conv2d(<span class="dv">64</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>,<span class="dv">3</span>), padding<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">1</span>), padding_mode<span class="op">=</span><span class="st">&#39;zeros&#39;</span>, stride<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block3_conv3 <span class="op">=</span> torch.nn.Conv2d(<span class="dv">64</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>,<span class="dv">3</span>), padding<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">1</span>), padding_mode<span class="op">=</span><span class="st">&#39;zeros&#39;</span>, stride<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block3_conv4 <span class="op">=</span> torch.nn.Conv2d(<span class="dv">64</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>,<span class="dv">3</span>), padding<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">1</span>), padding_mode<span class="op">=</span><span class="st">&#39;zeros&#39;</span>, stride<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># block up 3</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block3_up1 <span class="op">=</span> torch.nn.ConvTranspose2d(<span class="dv">64</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>,<span class="dv">3</span>), padding<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">1</span>), padding_mode<span class="op">=</span><span class="st">&#39;zeros&#39;</span>, stride<span class="op">=</span><span class="dv">2</span>, output_padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block3_up2 <span class="op">=</span> torch.nn.Conv2d(<span class="dv">64</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>,<span class="dv">3</span>), padding<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">1</span>), padding_mode<span class="op">=</span><span class="st">&#39;zeros&#39;</span>, stride<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># block up 2</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block2_up1 <span class="op">=</span> torch.nn.ConvTranspose2d(<span class="dv">64</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>,<span class="dv">3</span>), padding<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">1</span>), padding_mode<span class="op">=</span><span class="st">&#39;zeros&#39;</span>, stride<span class="op">=</span><span class="dv">2</span>, output_padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block2_up2 <span class="op">=</span> torch.nn.Conv2d(<span class="dv">64</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>,<span class="dv">3</span>), padding<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">1</span>), padding_mode<span class="op">=</span><span class="st">&#39;zeros&#39;</span>, stride<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># block up 1</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block1_up1 <span class="op">=</span> torch.nn.ConvTranspose2d(<span class="dv">64</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>,<span class="dv">3</span>), padding<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">1</span>), padding_mode<span class="op">=</span><span class="st">&#39;zeros&#39;</span>, stride<span class="op">=</span><span class="dv">2</span>, output_padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block1_up2 <span class="op">=</span> torch.nn.Conv2d(<span class="dv">64</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>,<span class="dv">3</span>), padding<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">1</span>), padding_mode<span class="op">=</span><span class="st">&#39;zeros&#39;</span>, stride<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># output</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv_output <span class="op">=</span> torch.nn.Conv2d(<span class="dv">64</span>, <span class="dv">1</span>, kernel_size<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">1</span>), padding<span class="op">=</span>(<span class="dv">0</span>,<span class="dv">0</span>), padding_mode<span class="op">=</span><span class="st">&#39;zeros&#39;</span>, stride<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>        <span class="co">#</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu <span class="op">=</span> torch.nn.ReLU()</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, alpha):</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>        b0 <span class="op">=</span> torch.cat([x, alpha[:,<span class="va">None</span>,<span class="va">None</span>,<span class="va">None</span>].repeat(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">32</span>, <span class="dv">32</span>)], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>        b1_c1 <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.block1_conv1(b0))</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>        b1_c2 <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.block1_conv2(b1_c1))</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>        b2_c1 <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.block2_conv1(b1_c2))</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>        b2_c2 <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.block2_conv2(b2_c1))</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>        b3_c1 <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.block3_conv1(b2_c2))</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>        b3_c2 <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.block3_conv2(b3_c1))</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>        b3_c3 <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.block3_conv3(b3_c2)) <span class="op">+</span> b3_c1</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>        b3_c4 <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.block3_conv4(b3_c3))</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>        u2_c1 <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.block3_up1(b3_c4)) <span class="op">+</span> b3_c3</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>        u2_c2 <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.block3_up2(u2_c1)) <span class="op">+</span> b2_c2</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>        u1_c1 <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.block2_up1(u2_c2)) <span class="op">+</span> b1_c2</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>        u1_c2 <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.block2_up2(u1_c1))</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>        u0_c1 <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.block1_up1(u1_c2)) <span class="op">+</span> b1_c1</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>        u0_c2 <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.block1_up2(u0_c1))</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.conv_output(u0_c2)</span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code></pre></div>
<p>We allocate the neural network and its optimizer:</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># allocating the neural network D</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> Unet().to(<span class="st">&#39;cuda&#39;</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>optimizer_D <span class="op">=</span> torch.optim.Adam(D.parameters(), lr<span class="op">=</span><span class="fl">0.0005</span>)</span></code></pre></div>
<h2 id="training">Training</h2>
<p>The training loop consists of sampling random <span
class="math inline"><em>x</em><sub>0</sub></span> and <span
class="math inline"><em>x</em><sub>1</sub></span>, blending them with
random <span class="math inline"><em>Œ±</em>‚ÄÑ‚àà‚ÄÑ[0,1]</span> to obtain
<span class="math inline"><em>x</em><sub><em>Œ±</em></sub></span>
samples, and training the network to predict <span
class="math inline"><em>x</em><sub>1</sub>‚ÄÖ‚àí‚ÄÖ<em>x</em><sub>0</sub></span>.
We train for 16 periods over the whole dataset.</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># training loop</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> period <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">16</span>):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> tqdm(dataloader, <span class="st">&quot;period &quot;</span> <span class="op">+</span> <span class="bu">str</span>(period)):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># get data      </span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        mnist <span class="op">=</span> <span class="op">-</span><span class="dv">1</span> <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>batch[<span class="dv">0</span>].to(<span class="st">&quot;cuda&quot;</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>        mnist <span class="op">=</span> torch.nn.functional.interpolate(mnist, size<span class="op">=</span>(<span class="dv">32</span>,<span class="dv">32</span>), mode<span class="op">=</span><span class="st">&#39;bilinear&#39;</span>, align_corners<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        x_0 <span class="op">=</span> torch.randn(batchsize, <span class="dv">1</span>, <span class="dv">32</span>, <span class="dv">32</span>, device<span class="op">=</span><span class="st">&quot;cuda&quot;</span>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        x_1 <span class="op">=</span> mnist            </span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        alpha <span class="op">=</span> torch.rand(batchsize, device<span class="op">=</span><span class="st">&quot;cuda&quot;</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        x_alpha <span class="op">=</span> (<span class="dv">1</span><span class="op">-</span>alpha[:,<span class="va">None</span>,<span class="va">None</span>,<span class="va">None</span>]) <span class="op">*</span> x_0 <span class="op">+</span> alpha[:,<span class="va">None</span>,<span class="va">None</span>,<span class="va">None</span>] <span class="op">*</span> x_1</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        <span class="co">#</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> torch.<span class="bu">sum</span>( (D(x_alpha, alpha) <span class="op">-</span> (x_1<span class="op">-</span>x_0))<span class="op">**</span><span class="dv">2</span> )</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        optimizer_D.zero_grad()</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>        optimizer_D.step()</span></code></pre></div>
<h2 id="sampling">Sampling</h2>
<p>Once the network is trained, we evaluate the mapping by starting from
random <span
class="math inline"><em>x</em><sub>0</sub>‚ÄÑ‚àº‚ÄÑ<em>p</em><sub>0</sub></span>
and moving the points along the direction predicted by the neural
network.</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># sampling loop</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># starting points x_alpha = x_0</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    x_0 <span class="op">=</span> torch.randn(batchsize, <span class="dv">1</span>, <span class="dv">32</span>, <span class="dv">32</span>, device<span class="op">=</span><span class="st">&quot;cuda&quot;</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    x_alpha <span class="op">=</span> x_0</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># loop</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    T <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> tqdm(<span class="bu">range</span>(T), <span class="st">&quot;sampling loop&quot;</span>):</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># current alpha value</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        alpha <span class="op">=</span> t <span class="op">/</span> T <span class="op">*</span> torch.ones(batchsize, device<span class="op">=</span><span class="st">&quot;cuda&quot;</span>)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># update </span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        x_alpha <span class="op">=</span> x_alpha <span class="op">+</span> <span class="dv">1</span><span class="op">/</span>T <span class="op">*</span> D(x_alpha, alpha)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># create result image</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        result <span class="op">=</span> np.zeros((<span class="dv">8</span><span class="op">*</span><span class="dv">32</span>, <span class="dv">8</span><span class="op">*</span><span class="dv">32</span>, <span class="dv">3</span>))         </span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">8</span>):</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">8</span>):</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>                tmp <span class="op">=</span> <span class="fl">0.5</span><span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>x_alpha[(i<span class="op">+</span><span class="dv">8</span><span class="op">*</span>j)<span class="op">%</span>batchsize, ...].repeat(<span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">1</span>).detach().cpu().clone().numpy()</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>                tmp <span class="op">=</span> np.swapaxes(tmp, <span class="dv">0</span>, <span class="dv">2</span>)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>                tmp <span class="op">=</span> np.swapaxes(tmp, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>                result[<span class="dv">32</span><span class="op">*</span>i:<span class="dv">32</span><span class="op">*</span>i<span class="op">+</span><span class="dv">32</span>, <span class="dv">32</span><span class="op">*</span>j:<span class="dv">32</span><span class="op">*</span>j<span class="op">+</span><span class="dv">32</span>, :] <span class="op">=</span> tmp          </span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        saveImage(<span class="st">&#39;generated_mnist_&#39;</span><span class="op">+</span><span class="bu">str</span>(t)<span class="op">+</span><span class="st">&#39;.png&#39;</span>, result)</span></code></pre></div>
<p>This is a GIF animation made with the exported images.</p>
<div style="display:flex; justify-content:center">
<div>
<center style="font-weight:800;margin-bottom: 0;">
<span class="math inline"><em>x</em><sub>0</sub></span>
</center>
<p><img style="width:100%;" src="./images/x0.png"></p>
</div>
<div>
¬†¬†¬†
</div>
<div>
<center style="font-weight:800;margin-bottom: 0;">
<span class="math inline"><em>x</em><sub><em>Œ±</em></sub></span>
</center>
<p><img style="width:100%;" src="./images/x_alpha.gif"></p>
</div>
<div>
¬†¬†¬†
</div>
<div>
<center style="font-weight:800;margin-bottom: 0;">
<span class="math inline"><em>x</em><sub>1</sub></span>
</center>
<p><img style="width:100%;" src="./images/x1.png"></p>
</div>
</div>
<h2 id="full-code">Full code</h2>
<p>You can find the full code <a
href="https://github.com/tchambon/posts/blob/main/iadb-MNIST/IADB_MNIST.py">here</a>.</p>
</main>

</body>
</html>
